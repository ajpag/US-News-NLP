summary(lme3)
lme4 <- lmer(mathgain ~ mathkind + ses + (1 | schoolid / classid), data = cl)
# log likelihood
logLik(lme4)
# model fit
summary(lme4)
# add variables
ses_sch <- cl %>% group_by(schoolid) %>% summarise(ses_sch = mean(ses))
ses_cls <- cl %>% group_by(schoolid, classid) %>% summarise(ses_cls = mean(ses))
mk_sch <- cl %>% group_by(schoolid) %>% summarise(mk_sch = mean(mathkind))
mk_cls <- cl %>%
group_by(schoolid, classid) %>%
summarise(mk_cls = mean(mathkind))
# add columns to main data
cl_new <- cl %>%
inner_join(ses_sch) %>%
inner_join(ses_cls) %>%
inner_join(mk_sch) %>%
inner_join(mk_cls)
# run model
lme5 <- lmer(mathgain ~ mathkind + ses + ses_sch + ses_cls + mk_sch + mk_cls +
(1 | schoolid / classid), data = cl_new)
# log likelihood
logLik(lme5)
# log likelihood
logLik(lme5)
# model fit
summary(lme5)
# not run
# setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/scripts")
library(httr)
library(jsonlite)
library(lubridate)
library(rvest)
library(tidyverse)
library(tidytext)
# API Documentation: https://documenter.getpostman.com/view/12365554/TVep87Q1#intro
# note: generate your own GNews API token and save it as a file called "api_token"
token <- colnames(read.csv("api_token"))
# parameters
topic <- "\"COVID-19\""
country <- "us"
language <- "en"
# article limit per API call under the free plan
limit <- "20"
source <- "cnn"
# date range
date_init <- as.Date("2020-01-01")
date_end <- as.Date("2021-04-09")
get_date_range <- function(date_start, date_end, start_end = "start") {
# get start or end of week for each date, given range of dates
# date_start: beginning date
# date_end: end date
# start_end: Get first or last date of the week. Values: "start" or "end"
dates <- integer(0)
class(dates) <- "Date"
i <- 1
while (date_start <= date_end) {
if (start_end == "start")
{dates[i] <- floor_date(date_start, unit = "week")}
else
{dates[i] <- ceiling_date(date_start, unit = "week") - 1}
date_start <- date_start + 7
i <- i + 1
}
return(dates)
}
# date ranges for API
dates_start <- get_date_range(date_init, date_end, start_end = "start")
dates_end <- get_date_range(date_init, date_end, start_end = "end")
get_news <- function(topic, country, language, date_from,
date_to, source, limit, token) {
# get GNews API response
response <- GET(paste0("https://gnewsapi.net/api/search",
"?q=", URLencode(topic),
"&country=", country,
"&language=", language,
"&from=", date_from,
"&to=", date_to,
"&inurl=", source,
"&limit=", limit,
"&api_token=", token),
type = "basic")
return(response)
}
# API call - one call per week
# check API page in https://gnewsapi.net/settings#/api to monitor progress
api_results <- list()
for (i in seq_along(dates_start)) {
api_results[[i]] <- get_news(topic, country, language,
dates_start[i], dates_end[i],
source, limit, token)
}
# convert to dataframe
news <- bind_rows(
lapply(
api_results, function(x) {
as.data.frame(fromJSON(content(x, "text"), flatten = TRUE))
}
)
)
# not run
setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/scripts")
library(httr)
library(jsonlite)
library(lubridate)
library(rvest)
library(tidyverse)
library(tidytext)
# API Documentation: https://documenter.getpostman.com/view/12365554/TVep87Q1#intro
# note: generate your own GNews API token and save it as a file called "api_token"
token <- colnames(read.csv("api_token"))
# parameters
topic <- "\"COVID-19\""
country <- "us"
language <- "en"
# article limit per API call under the free plan
limit <- "20"
source <- "cnn"
# date range
date_init <- as.Date("2020-01-01")
date_end <- as.Date("2021-04-09")
get_date_range <- function(date_start, date_end, start_end = "start") {
# get start or end of week for each date, given range of dates
# date_start: beginning date
# date_end: end date
# start_end: Get first or last date of the week. Values: "start" or "end"
dates <- integer(0)
class(dates) <- "Date"
i <- 1
while (date_start <= date_end) {
if (start_end == "start")
{dates[i] <- floor_date(date_start, unit = "week")}
else
{dates[i] <- ceiling_date(date_start, unit = "week") - 1}
date_start <- date_start + 7
i <- i + 1
}
return(dates)
}
# date ranges for API
dates_start <- get_date_range(date_init, date_end, start_end = "start")
dates_end <- get_date_range(date_init, date_end, start_end = "end")
get_news <- function(topic, country, language, date_from,
date_to, source, limit, token) {
# get GNews API response
response <- GET(paste0("https://gnewsapi.net/api/search",
"?q=", URLencode(topic),
"&country=", country,
"&language=", language,
"&from=", date_from,
"&to=", date_to,
"&inurl=", source,
"&limit=", limit,
"&api_token=", token),
type = "basic")
return(response)
}
# API call - one call per week
# check API page in https://gnewsapi.net/settings#/api to monitor progress
api_results <- list()
for (i in seq_along(dates_start)) {
api_results[[i]] <- get_news(topic, country, language,
dates_start[i], dates_end[i],
source, limit, token)
}
# convert to dataframe
news <- bind_rows(
lapply(
api_results, function(x) {
as.data.frame(fromJSON(content(x, "text"), flatten = TRUE))
}
)
)
# note: generate your own GNews API token and save it as a file called "api_token"
token <- colnames(read.csv("../api_token"))
getwd()
# note: generate your own GNews API token and save it as a file called "api_token"
token <- colnames(read.csv("./api_token"))
# not run
setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/scripts")
library(httr)
library(jsonlite)
library(lubridate)
library(rvest)
library(tidyverse)
library(tidytext)
# note: generate your own GNews API token and save it as a file called "api_token"
token <- colnames(read.csv("../api_token"))
cd ..
# note: generate your own GNews API token and save it as a file called "api_token"
token <- colnames(read.csv("api_token"))
token
# not run
setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/scripts")
library(httr)
library(jsonlite)
library(lubridate)
library(rvest)
library(tidyverse)
library(tidytext)
# API Documentation: https://documenter.getpostman.com/view/12365554/TVep87Q1#intro
# note: generate your own GNews API token and save it as a file called "api_token"
token <- colnames(read.csv("api_token"))
# not run
setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/scripts")
library(httr)
library(jsonlite)
library(lubridate)
library(rvest)
library(tidyverse)
library(tidytext)
# note: generate your own GNews API token and save it as a file called "api_token"
token <- colnames(read.csv("api_token"))
token
# parameters
topic <- "\"COVID-19\""
country <- "us"
language <- "en"
# article limit per API call under the free plan
limit <- "20"
source <- "cnn"
# date range
date_init <- as.Date("2020-01-01")
date_end <- as.Date("2021-04-09")
get_date_range <- function(date_start, date_end, start_end = "start") {
# get start or end of week for each date, given range of dates
# date_start: beginning date
# date_end: end date
# start_end: Get first or last date of the week. Values: "start" or "end"
dates <- integer(0)
class(dates) <- "Date"
i <- 1
while (date_start <= date_end) {
if (start_end == "start")
{dates[i] <- floor_date(date_start, unit = "week")}
else
{dates[i] <- ceiling_date(date_start, unit = "week") - 1}
date_start <- date_start + 7
i <- i + 1
}
return(dates)
}
# date ranges for API
dates_start <- get_date_range(date_init, date_end, start_end = "start")
dates_end <- get_date_range(date_init, date_end, start_end = "end")
dates_start
dates_end
get_news <- function(topic, country, language, date_from,
date_to, source, limit, token) {
# get GNews API response
response <- GET(paste0("https://gnewsapi.net/api/search",
"?q=", URLencode(topic),
"&country=", country,
"&language=", language,
"&from=", date_from,
"&to=", date_to,
"&inurl=", source,
"&limit=", limit,
"&api_token=", token),
type = "basic")
return(response)
}
api_results <- list()
for (i in seq_along(dates_start)) {
api_results[[i]] <- get_news(topic, country, language,
dates_start[i], dates_end[i],
source, limit, token)
}
# convert to dataframe
news <- bind_rows(
lapply(
api_results, function(x) {
as.data.frame(fromJSON(content(x, "text"), flatten = TRUE))
}
)
)
api_results
api_results[[67]]
api_results[[66]]
api_results[[64]]
api_results[[60]]
api_results[[10]]
api_results[[20]]
api_results[[21]]
api_results[[22]]
api_results[[23]]
api_results[[40]]
api_results[[50]]
api_results[[51]]
api_results[[52]]
api_results[[53]]
api_results[[55]]
api_results[[56]]
as.data.frame(fromJSON(content(api_results[[5]], "text"), flatten = TRUE))
as.data.frame(fromJSON(content(api_results[[55]], "text"), flatten = TRUE))
as.data.frame(fromJSON(content(api_results[[56]], "text"), flatten = TRUE))
news <- bind_rows(
lapply(
api_results, function(x) {
as.data.frame(fromJSON(content(x, "text"), flatten = TRUE))
}
)
)
news
dim(news)
api_results[[56]]
api_results[c(1:55)]
# API not working starting with element 56 (week of 4/12/2021)
# subset up through that point for now
api_results_sub <- api_results[c(1:55)]
# convert to dataframe
news <- bind_rows(
lapply(
api_results_sub, function(x) {
as.data.frame(fromJSON(content(x, "text"), flatten = TRUE))
}
)
)
length(api_results_sub)
api_results[[5]]
api_results[[55]]
api_results[[54]]
api_results[[40]]
api_results[[41]]
api_results[[42]]
api_results[[43]]
attributes(api_results[[1]])
api_results[[1]]$response
api_results[[1]]$class
t <- api_results[[1]]
class(t)
names(t)
t$status_code
for (api in api_results) {print(api$status_code)}
# only include API results that return a valid status code
lapply(api_results, function(x) x$status_code)
# only include API results that return a valid status code
sapply(api_results, function(x) x$status_code)
# only include API results that return a valid status code
api_status_codes <- sapply(api_results, function(x) x$status_code)
api_status_codes
which(api_status_codes == 200)
# only include API results that return a valid status code
api_status_codes <- which(sapply(api_results, function(x) x$status_code) == 200)
api_status_codes
api_results(api_status_codes)
api_results(c(api_status_codes))
api_results[c(api_status_codes)]
api_results[api_status_codes]
api_status_codes
length(api_status_codes)
length(api_results[api_status_codes])
# convert to dataframe
news <- bind_rows(
lapply(
api_results(api_status_codes), function(x) {
as.data.frame(fromJSON(content(x, "text"), flatten = TRUE))
}
)
)
# convert to dataframe
news <- bind_rows(
lapply(
api_results[api_status_codes], function(x) {
as.data.frame(fromJSON(content(x, "text"), flatten = TRUE))
}
)
)
news
dim(news)
get_cnn_text <- function(url)
{
# pull CNN article text given url.
html_ <- read_html(url)
# get first sentence of article text
df_first <- data.frame(
text = html_ %>%
xml_find_all("//p[contains(@class, 'zn-body__paragraph speakable')]") %>%
html_text(trim = TRUE),
stringsAsFactors = FALSE
)
# get remainder of article text
df_text <- data.frame(
text = html_ %>%
xml_find_all("//div[contains(@class, 'zn-body__paragraph')]") %>%
html_text(trim = TRUE),
stringsAsFactors = FALSE
)
# get full article text
final_text <- bind_rows(df_first, df_text)
return(paste(final_text))
}
# full article text
text <- lapply(news$articles.article_url, get_cnn_text)
text
text <- get_cnn_text(news$articles.article_url)
# full article text
# this can take time. Find a more efficient way to pull this.
text <- lapply(news$articles.article_url, get_cnn_text)
# add full text to df
news$text <- NA
for (i in seq_along(text)) {news$text[i] <- text[[i]]}
news$text <- if_else(news$text == "character(0)",
"N/A",
# remove combine syntax, i.e. "c()"
substring(news$text, 4, nchar(news$text) - 2))
# write to csv
write_csv(news, file = "../data/news_data_cnn.csv")
get_reuters_text <- function(url) {
df <- data.frame(text = read_html(url) %>%
xml_find_all(
"//p[contains(@class, 'Paragraph-paragraph-2Bgue ArticleBody-para-TD_9x')]"
) %>%
html_text(trim = TRUE),
stringsAsFactors = FALSE
)
return(paste(df))
}
source_r <- "reuters"
# API call - one call per week
# check API page in https://gnewsapi.net/settings#/api to monitor progress
api_results_r <- list()
for (i in seq_along(dates_start)) {
api_results_r[[i]] <- get_news(topic, country, language,
dates_start[i], dates_end[i],
source_r, limit, token)
}
api_results_r
# only include API results that return a valid status code
api_status_codes_r <- which(sapply(api_results_r, function(x) x$status_code) == 200)
length(api_status_codes_r)
news_r <- bind_rows(
lapply(
api_results_r[api_status_codes_r], function(x) {
as.data.frame(fromJSON(content(x, "text"), flatten = TRUE))
}
)
)
dim(news_r)
news_r
get_reuters_text <- function(url) {
df <- data.frame(text = read_html(url) %>%
xml_find_all(
"//p[contains(@class, 'Paragraph-paragraph-2Bgue ArticleBody-para-TD_9x')]"
) %>%
html_text(trim = TRUE),
stringsAsFactors = FALSE
)
return(paste(df))
}
Vectorize(get_reuters_text, vectorize.args = url)
Vectorize(get_reuters_text, vectorize.args = "url")
v_get_reuters_text <- Vectorize(get_reuters_text, vectorize.args = "url")
v_get_reuters_text(news_r$articles.article_url)
# get full article text
test <- v_get_reuters_text(news_r$articles.article_url)
proc.time()
proc.time
proc.time()
t0 <- proc.time()
t1 <- proc.time()
t1 - t0
(t1 - t0) / 60
paste("vectorized code", (t1 - t0) / 60)
(t1 - t0) / 60
t1
t0
# time code
t0 <- proc.time()
# get full article text
test <- v_get_reuters_text(news_r$articles.article_url)
t1 <- proc.time()
# time elapsed
t1 - t0 / 60
t2 <- proc.time()
# full article text
text_r <- lapply(news_r$articles.article_url, get_reuters_text)
t3 <- proc.time()
(t3 - t2) / 60
# try vectorizing function
v_get_reuters_text <- Vectorize(get_reuters_text, vectorize.args = "url")
# time code
t0 <- proc.time()
# get full article text
test <- v_get_reuters_text(news_r$articles.article_url)
# time elapsed
(proc.time() - t0) / 60
v_get_reuters_text()
# try vectorizing function
v_get_reuters_text <- Vectorize(get_reuters_text, vectorize.args = "url")
# time code
t0 <- proc.time()
# get full article text
test <- v_get_reuters_text(news_r$articles.article_url)
# time elapsed
(proc.time() - t0) / 60
t1 <- proc.time()
# full article text
text_r <- lapply(news_r$articles.article_url, get_reuters_text)
(proc.time() - t1) / 60
length(text_r)
dim(news_r)
news_r$text <- NA
for (i in seq_along(text)) {news_r$text[i] <- text_r[[i]]}
news_r$text <- if_else(news_r$text == "character(0)",
"N/A",
# remove combine syntax, i.e. "c()"
substring(news_r$text, 4, nchar(news_r$text) - 2))
dim(news_r)
# write to csv
write_csv(news_r, file = "../data/news_data_reuters.csv")
