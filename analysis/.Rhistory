logLik(lme2)
summary(lme2)
lme1a <- lmer(mathkind ~ (1 | schoolid / classid), data = cl)
# log likelihood
logLik(lme1a)
summary(lme1a)
anova(lme1, lme1a)
?anova
anova(lme1, lme1a)
anova(lme1, lme1a, refit = False)
anova(lme1, lme1a, refit = FALSE)
anova(lme1, lme1a, refit = TRUE)
anova(lme1, lme1a, refit = FALSE)
str(cl)
# generate 1st grade math scores
cl$math1st <- cl$mathkind = cl$mathgain
# generate 1st grade math scores
cl$math1st <- cl$mathkind + cl$mathgain
head(cl)
# model
# unconditional means model
lme2 <- lmer(math1st ~ (1 | schoolid), data = cl)
# log likelihood
logLik(lme2)
lme2a <- lmer(math1st ~ (1 | schoolid / classid), data = cl)
# log likelihood
logLik(lme2a)
summary(lme2a)
anova(lme2, lme2a, refit = FALSE)
lme3 <- lmer(math1st ~ ses + sex + minority + (1 | schoolid / classid), data = cl)
logLik(lme3)
lme3 <- lmer(math1st ~ ses + sex + minority + (1 | schoolid), data = cl)
logLik(lme3)
lme3 <- lmer(math1st ~ ses + sex + minority + (1 | schoolid), data = cl)
logLik(lme3)
lme3 <- lmer(math1st ~ ses + sex + minority + (1 | schoolid / classid), data = cl)
logLik(lme3)
lme3 <- lmer(math1st ~ ses + sex + minority + (1 | schoolid), data = cl)
logLik(lme3)
summary(lme3)
lme3a <- lmer(math1st ~ ses + sex + minority + (1 | schoolid), data = cl)
logLik(lme3a)
summary(lme3a)
lme3b <- lmer(math1st ~ ses + sex + minority + (1 | schoolid / classid),
data = cl)
logLik(lme3b)
summary(lme3b)
anova(lme3a, lme3b)
anova(lme3a, lme3b, refit = FALSE)
lmer::anova
colnames(cl)
lme4a <- lmer(math1st ~ ses + sex + minority + yearstea + (1 | schoolid / classid),
data = cl)
logLik(lme4a)
summary(lme4a)
cl$yt_sq <- cl$yearstea^2
cl$yt_cyb <- cl$yearstea^3
head(cl)
head(cl, 20)
12.54^2
12.54^3
lme4d <- lmer(math1st ~ ses + sex + minority + yearstea + yt_sq + yt_cub +
(1 | schoolid / classid),
data = cl)
logLik(lme4d)
lme4d <- lmer(math1st ~ ses + sex + minority + yearstea + yt_sq + yt_cub +
(1 | schoolid / classid),
data = cl)
cl$yt_sq <- cl$yearstea^2
cl$yt_cub <- cl$yearstea^3
lme4d <- lmer(math1st ~ ses + sex + minority + yearstea + yt_sq + yt_cub +
(1 | schoolid / classid),
data = cl)
logLik(lme4d)
summary(lme4d)
anova(lme4a, lme4d)
anova(lme4a, lme4d, refit = FALSE)
hea(cl, 20)
head(cl, 20)
unique(cl$ses)
len(unique(cl$ses))
length(unique(cl$ses))
length(unique(cl$schoolid))
length(unique(cl$classid))
unique(cl$childid)
cl$classid
library(foreign)
library(lme4)
library(lmerTest)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE, tidy.opts = list(width.cutoff = 60),
tidy = TRUE, warning = FALSE)
# load data
cl <- read.dta("classroom.dta")
# convert to factor
cols_factor <- c("sex", "minority", "classid", "schoolid", "childid")
cl[cols_factor] <- lapply(cl[cols_factor], factor)
# unconditional means model
lme1 <- lmer(mathkind ~ (1 | schoolid), data = cl)
# log likelihood
logLik(lme1)
summary(lme1)
lme1a <- lmer(mathkind ~ (1 | schoolid / classid), data = cl)
# log likelihood
logLik(lme1a)
summary(lme1a)
anova(lme1, lme1a, refit = FALSE)
# generate 1st grade math scores
cl$math1st <- cl$mathkind + cl$mathgain
# model
# unconditional means model
lme2 <- lmer(math1st ~ (1 | schoolid), data = cl)
# log likelihood
logLik(lme2)
lme2a <- lmer(math1st ~ (1 | schoolid / classid), data = cl)
# log likelihood
logLik(lme2a)
summary(lme2a)
anova(lme2, lme2a, refit = FALSE)
lme3a <- lmer(math1st ~ ses + sex + minority + (1 | schoolid), data = cl)
logLik(lme3a)
summary(lme3a)
lme3b <- lmer(math1st ~ ses + sex + minority + (1 | schoolid / classid),
data = cl)
logLik(lme3b)
summary(lme3b)
anova(lme3a, lme3b, refit = FALSE)
lme4a <- lmer(math1st ~ ses + sex + minority + yearstea + (1 | schoolid / classid),
data = cl)
logLik(lme4a)
summary(lme4a)
cl$yt_sq <- cl$yearstea^2
cl$yt_cub <- cl$yearstea^3
lme4d <- lmer(math1st ~ ses + sex + minority + yearstea + yt_sq + yt_cub +
(1 | schoolid / classid),
data = cl)
logLik(lme4d)
summary(lme4d)
anova(lme4a, lme4d, refit = FALSE)
lme4a <- lmer(math1st ~ ses + sex + minority + yearstea + (1 | schoolid / classid),
data = cl, REML = FALSE)
logLik(lme4a)
lme4a <- lmer(math1st ~ ses + sex + minority + yearstea + (1 | schoolid / classid),
data = cl)
logLik(lme4a)
lme4a <- lmer(math1st ~ ses + sex + minority + yearstea + (1 | schoolid / classid),
data = cl, REML = FALSE)
logLik(lme4a)
```{r}
summary(lme4a)
cl$yt_sq <- cl$yearstea^2
cl$yt_cub <- cl$yearstea^3
lme4d <- lmer(math1st ~ ses + sex + minority + yearstea + yt_sq + yt_cub +
(1 | schoolid / classid),
data = cl,
REML = FALSE)
logLik(lme4d)
lme4d <- lmer(math1st ~ ses + sex + minority + yearstea + yt_sq + yt_cub +
(1 | schoolid / classid),
data = cl)
logLik(lme4d)
lme4d <- lmer(math1st ~ ses + sex + minority + yearstea + yt_sq + yt_cub +
(1 | schoolid / classid),
data = cl,
REML = FALSE)
logLik(lme4d)
summary(lme4d)
anova(lme4a, lme4d, refit = FALSE)
anova(lme4a, lme4d)
anova(lme4a, lme4d, refit = FALSE)
library(foreign)
library(lme4)
library(lmerTest)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE, tidy.opts = list(width.cutoff = 60),
tidy = TRUE, warning = FALSE)
anova(lme1, lme1a, refit = TRUE)
anova(lme1, lme1a, refit = FALSE)
anova(lme2, lme2a, refit = TRUE)
anova(lme2, lme2a, refit = FALSE)
anova(lme3a, lme3b, refit = TRUE)
anova(lme3a, lme3b, refit = FALSE)
rm(list = ls())
ls()
rm(list = ls())
# not run
setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/analysis")
library(anytime)
library(dplyr)
library(ggplot2)
library(lubridate)
library(readr)
library(tidyr)
library(tidytext)
library(topicmodels)
seed <- 14
# directories
figures_dir <- "../figures/"
data_dir <- "../data/"
# read news data
articles <- read_csv(paste0(data_dir, "news_all.csv"))  %>%
mutate(document = as.character(row_number()))
# filter out international Reuters sources
news_source_exclude <- c("Reuters Africa", "Reuters Australia",
"Reuters India", "Reuters UK", "Reuters.com")
# remove international Reuters articles
articles <- filter(articles, !articles.source_name %in% news_source_exclude)
# Purpose: Engineer new features for classification modeling
################################ Topic Modeling ################################
# also removed top COVID-19 high-frequency words
# the articles were already filtered to COVID-19 articles via GNews API
stop_words_custom <- tibble(word = c("n", "2w7hx9t", "202f", "comma",
"covid", "19", "coronavirus",
"virus", "health", "people",
"bbc", "reuters", "cnn", "wsj"))
# word frequencies
term_freq <- articles %>%
mutate(document = row_number()) %>%
unnest_tokens(word, text) %>%
group_by(document, word) %>% summarise(n = n()) %>%
anti_join(stop_words, by = c(word = "word")) %>%
anti_join(stop_words_custom, by = c(word = "word"))
# document-term matrix
news_dtm <- term_freq %>% cast_dtm(document, word, n)
#######
# VEM #
#######
# fit a topic model, VEM method
# try different k and fitting methods
topics <- 7
news_lda <- LDA(news_dtm, k = topics, control = list(seed = seed))
# plot top terms
plot_lda <- function(lda_df, n_terms = 10, plot_title = "") {
# Plots top n terms from an LDA object
# lda_df: top term
# n_terms: number of top terms to plot per topic
# per-topic per-word probabilities
news_topics <- tidy(lda_df, matrix = "beta")
top_terms <- news_topics %>%
group_by(topic) %>%
top_n(n_terms, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# plot top terms
p <- top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free", ncol = 2) +
coord_flip() +
labs(title = plot_title)
return(p)
}
# plot top terms per topic
p1 <- plot_lda(news_lda, 10, "Topic Modeling (LDA VEM): Top Terms")
p1
# save plot
ggsave(plot = p1, file = paste0(figures_dir, "lda_top_terms.png"),
height = 10, width = 10)
# topic probabilities by document
lda_probs <- tidy(news_lda, matrix = "gamma") %>%
pivot_wider(names_from = topic,
values_from = gamma,
names_prefix = "prob_topic_")
# add to dataframe
articles <- articles %>% left_join(lda_probs)
#########
# Gibbs #
#########
# # try Gibbs method
# news_lda_g <- LDA(news_dtm, k = topics, method = "Gibbs",
#                 control = list(seed = seed))
#
# # plot top terms per topic
# p2 <- plot_lda(news_lda_g, 10, "Topic Modeling (LDA Gibbs): Top Terms")
#
# # save plot
# ggsave(plot = p2, file = paste0(figures_dir, "lda_top_terms_gibbs.png"),
#        height = 10, width = 10)
#
# # add topic probabilities to dataframe
# # topic probabilities
# lda_probs_g <- tidy(news_lda, matrix = "gamma")
# colnames(lda_probs_g) <- paste0("topic_", colnames(lda_probs_g), "_prob_gibbs")
#
# # add topic probabilities to dataframe
# # topic probabilities by document
# lda_probs_g <- tidy(news_lda_g, matrix = "gamma") %>%
#   pivot_wider(names_from = topic,
#               values_from = gamma,
#               names_prefix = "topic_gibbs_")
#
# # add to dataframe
# articles <- articles %>% left_join(lda_probs_g)
################################# Sentiment Features ###########################
# Calculate average sentiment per article
##################
# Tokenize Words #
##################
tokenize_words <- function(df, text) {
# tokenize text by word and add sentiments
# text: column to tokenize
words <- df %>%
unnest_tokens(word, text) %>%
left_join(get_sentiments(lexicon = "bing") %>%
mutate(sentiment_bing = sentiment) %>%
select(-sentiment)) %>%
left_join(get_sentiments(lexicon = "afinn") %>%
mutate(sentiment_afinn = value) %>%
select(-value)) %>%
left_join(get_sentiments(lexicon = "loughran") %>%
mutate(sentiment_loughran = sentiment) %>%
select(-sentiment)) %>%
left_join(get_sentiments(lexicon = "nrc") %>%
mutate(sentiment_nrc = sentiment) %>%
select(-sentiment))
return(words)
}
#####################
# Sentiment by word #
#####################
# tokenize text
words <- tokenize_words(articles, text)
# calculate average afinn sentiment per word and article
avg_sentiment_afinn_word <- words %>%
select(document, sentiment_afinn) %>%
drop_na() %>%
group_by(document) %>%
summarise(avg_sentiment_afinn_word = mean(sentiment_afinn)) %>%
mutate(document = as.character(document))
# word count per article
word_count <- words %>%
group_by(document) %>%
mutate(document = as.character(document)) %>%
summarise(word_count = n())
# word count with mapped sentiment
word_count_sentiment <- words %>%
group_by(document) %>%
inner_join(get_sentiments("afinn")) %>%
mutate(document = as.character(document)) %>%
summarise(word_count_sentiment = n())
# add to dataframe
articles <- articles %>% left_join(avg_sentiment_afinn_word)
#########################
# Sentiment by sentence #
#########################
# tokenize by sentence and word
sentences <- articles %>%
unnest_tokens(sentence, text, token = "sentences") %>%
group_by(document) %>%
mutate(sentence_num = row_number()) %>%
ungroup() %>%
unnest_tokens(word, sentence)
# add average sentiment by sentence
sentences_sentiment <- sentences %>%
inner_join(get_sentiments("afinn")) %>%
group_by(document) %>%
summarise(avg_sentiment_afinn_sent = mean(value)) %>%
mutate(document = as.character(document))
# add to dataframe
# also add word counts
articles <- articles %>%
left_join(sentences_sentiment) %>%
left_join(word_count) %>%
left_join(word_count_sentiment)
##################### Plot Features ############################################
# inspect differences between sentiment by word and sentence across data sources
articles %>%
pivot_longer(cols = c(avg_sentiment_afinn_word,
avg_sentiment_afinn_sent)) %>%
ggplot(aes(x = value, fill = name)) +
geom_histogram() +
facet_wrap(~articles.source_name) +
labs(title = "Sentiment by Article") +
theme(legend.position = "bottom")
# plot topic probabilities by news source
p_topic <- articles %>%
select(articles.source_name | contains("topic")) %>%
pivot_longer(cols = -articles.source_name) %>%
drop_na() %>%
group_by(name, articles.source_name) %>%
summarise(topic_prob = mean(value)) %>%
ggplot(aes(x = articles.source_name, y = topic_prob, fill = articles.source_name)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~name) +
theme(legend.position = "bottom") +
labs(title = "Topic Probabilities by News Source")
ggsave(plot = p_topic, file = paste0(figures_dir, "topic_probabilities_news_source.png"),
height = 10, width = 10)
############################### TF-IDF #########################################
# drop for now.. this would add too many columns to the dataframe
# # prep data for tf_idf
# word_counts_ <- words %>%
#   group_by(document, word) %>%
#   summarise(n = n())
#
# # get tf-idf
# tf_idf <- bind_tf_idf(word_counts_, document, word, n)
########################## Other features #######################################
# add hour published and day of week published
articles <- articles %>%
mutate(published_hour_et = hour(anytime(articles.published_timestamp)),
published_dow = weekdays(articles$articles.published_datetime))
# add Jazmine's keyword sentiment features
keyword_features <- read_csv(paste0(data_dir, "sentiment_by_words_new_features.csv"))
# join datasets and remove redundant columns
articles <- articles %>%
left_join(keyword_features %>%
select(-c(articles.title, text, articles.published_datetime)),
by = c("document" = "article.no"))
# write to csv
write_csv(articles, paste0(data_dir, "news_model_input.csv"))
dim(aarticles)
dim(articles)
glimpse(keyword_features)
glimpse(article)
glimpse(articles)
# join datasets and remove redundant columns
articles <- articles %>%
left_join(keyword_features %>%
select(-c(articles.title, text, articles.published_datetime)) %>%
mutate(article.no = as.factor(article.no)),
by = c("document" = "article.no"))
colnames(articles)
dim(articles)
# write to csv
write_csv(articles, paste0(data_dir, "news_model_input.csv"))
p1
plot_lda(news_lda, 20, "Topic Modeling (LDA VEM): Top Terms")
news_topics <- tidy(news_lda, matrix = "beta")
news_topics %>%
group_by(topic) %>%
top_n(20, beta) %>%
ungroup() %>%
arrange(topic, -beta)
news_topics %>%
group_by(topic) %>%
top_n(20, beta) %>%
ungroup() %>%
arrange(topic, -beta) %>% print(n = 100)
news_topics %>%
group_by(topic) %>%
top_n(20, beta) %>%
ungroup() %>%
arrange(topic, -beta) %>% print(n = 200)
# not run
# setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/analysis")
library(anytime) # epoch to datetime
library(dplyr)
library(caret) # Gradient Boosting Machine
library(lubridate)
library(readr)
library(pROC) # multi-class ROC and AUC
library(xgboost)
seed <- 14
set.seed(seed)
# Purpose: Predict the news source
data_dir <- "../data/"
figures_dir <- "../figures/"
articles <- read_csv(paste0(data_dir, "news_model_input.csv"))
# filter to columns for modeling
articles_input <- articles %>%
select(c(document, articles.source_name, avg_sentiment_afinn_word,
articles.published_timestamp, articles.published_datetime,
avg_sentiment_afinn_sent, word_count, word_count_sentiment,
published_hour_et, published_dow) |
contains("topic")) %>%
drop_na()
################### Train / Test Split #########################################
# train / test split
test_rate <- .2
articles_train <- articles_input %>% sample_frac(size = 1 - test_rate)
articles_test <- articles_input[-c(pull(articles_train %>% select(document))), ]
# inspect distribution of news source
# make sure there is no imbalance across news sources
bind_rows(articles_train %>% mutate(set = "train"),
articles_test %>% mutate(set = "test")) %>%
group_by(articles.source_name, set) %>%
summarise(article_count = n()) %>%
ggplot(aes(x = articles.source_name, y = article_count)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~set)
###################### Gradient Boosted Machine ################################
#############
# Model Fit #
#############
# fit model
gbm_fit <- train(articles.source_name ~ . - articles.published_datetime
- articles.published_timestamp
- document
- published_hour_et,
data = articles_train,
method = "gbm")
# plot variable importance
varImp(gbm_fit)
# predictions on test set
gbm_preds <- predict(gbm_fit, articles_test)
# probabilities on test set
gbm_prob <- predict(gbm_fit, articles_test, type = "prob")
######################
# Validation Metrics #
######################
gbm_accuracy <- mean(gbm_preds == articles_test$articles.source_name)
paste("GBM Accuracy:", round(gbm_accuracy, 3))
varImp()
# plot variable importance
varImp(gbm_fit)
library(gbm) # need for feature importance
# plot variable importance
varImp(gbm_fit)
# plot variable importance
varImp(gbm_fit, scale = FALSE)
