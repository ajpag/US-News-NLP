# combine articles
articles %>% bind_rows(articles_cnn_reuters, articles_bbc_wsj)
# combine articles
articles %>% bind_rows(articles_cnn_reuters, articles_bbc_wsj) %>% dim
dim(articles_cnn_reuters)
dim(articles_bbc_wsj)
# read wsj and bbc
articles_bbc_wsj <- bind_rows(read_csv("../data/news_wsj.csv"),
read_csv("../data/news_bbc.csv"))
dim(articles_bbc_wsj)
articles_bbc_wsj %>%
select(-c(X1, X.1, X)) %>% dim
# remove first three columns and consolidate source names
articles_bbc_wsj <- articles_bbc_wsj %>%
select(-c(X1, X.1, X)) %>%
mutate(if_else(articles.source_name == "Wall Street Journal",
"The Wall Street Journal",
articles.source_name))
dim(articles_bbc_wsj)
colnames(articles_bbc_wsj)
# remove first three columns and consolidate source names
articles_bbc_wsj <- articles_bbc_wsj %>%
select(-c(X1, X.1, X)) %>%
mutate(articles.source_name =
if_else(articles.source_name == "Wall Street Journal",
"The Wall Street Journal",
articles.source_name))
# read wsj and bbc
articles_bbc_wsj <- bind_rows(read_csv("../data/news_wsj.csv"),
read_csv("../data/news_bbc.csv"))
# remove first three columns and consolidate source names
articles_bbc_wsj <- articles_bbc_wsj %>%
select(-c(X1, X.1, X)) %>%
mutate(articles.source_name =
if_else(articles.source_name == "Wall Street Journal",
"The Wall Street Journal",
articles.source_name))
dim(articles_bbc_wsj)
# combine articles
articles %>% bind_rows(articles_cnn_reuters, articles_bbc_wsj) %>% dim
# combine articles
articles <- bind_rows(articles_cnn_reuters, articles_bbc_wsj)
# number of articles by source
articles %>%
group_by(articles.source_name) %>%
summarise(articles = n()) %>%
arrange(desc(articles))
tokenize_words <- function(df) {
# tokenize words and add sentiments given news dataframe
words <- df %>%
unnest_tokens(word, text) %>%
left_join(get_sentiments(lexicon = "bing") %>%
mutate(sentiment_bing = sentiment) %>%
select(-sentiment)) %>%
left_join(get_sentiments(lexicon = "afinn") %>%
mutate(sentiment_afinn = as.factor(value)) %>%
select(-value)) %>%
left_join(get_sentiments(lexicon = "loughran") %>%
mutate(sentiment_loughran = sentiment) %>%
select(-sentiment)) %>%
left_join(get_sentiments(lexicon = "nrc") %>%
mutate(sentiment_nrc = sentiment) %>%
select(-sentiment))
return(words)
}
# baseline: tokenize words
words <- tokenize_words(articles)
# words per article
words %>%
group_by(articles.source_name, articles.article_url) %>%
summarise(word_count = n()) %>%
ggplot(aes(x = word_count, fill = articles.source_name)) +
geom_histogram(position = "dodge") +
labs(title = "Word Count Distribution")
# re-shape for plotting
words_long <- words %>%
pivot_longer(cols = contains("sentiment"),
names_to = "lexicon",
values_to = "sentiment")
# top words with a sentiment
words_long %>%
select(word, sentiment, articles.source_name) %>%
drop_na() %>%
group_by(word, articles.source_name) %>%
summarise(count = n()) %>%
group_by(articles.source_name) %>%
mutate(pct_of_total = count / sum(count)) %>%
arrange(desc(pct_of_total)) %>%
head(30) %>%
ggplot(aes(x = reorder(word, pct_of_total), y = pct_of_total)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
labs(title = "Top Words (with a sentiment)")
# top words with a sentiment
words_long %>%
select(word, sentiment, articles.source_name) %>%
drop_na() %>%
group_by(word, articles.source_name) %>%
summarise(count = n()) %>%
group_by(articles.source_name) %>%
mutate(pct_of_total = count / sum(count)) %>%
arrange(desc(pct_of_total)) %>%
head(30) %>%
ggplot(aes(x = reorder(word, pct_of_total), y = pct_of_total)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() + %>%
facet_wrap(~articles.source_name) %>%
labs(title = "Top Words (with a sentiment)")
# top words with a sentiment
words_long %>%
select(word, sentiment, articles.source_name) %>%
drop_na() %>%
group_by(word, articles.source_name) %>%
summarise(count = n()) %>%
group_by(articles.source_name) %>%
mutate(pct_of_total = count / sum(count)) %>%
arrange(desc(pct_of_total)) %>%
head(30) %>%
ggplot(aes(x = reorder(word, pct_of_total), y = pct_of_total)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
facet_wrap(~articles.source_name) +
labs(title = "Top Words (with a sentiment)")
# top words with a sentiment
words_long %>%
select(word, sentiment, articles.source_name) %>%
drop_na() %>%
group_by(word, articles.source_name) %>%
summarise(count = n()) %>%
group_by(articles.source_name) %>%
mutate(pct_of_total = count / sum(count)) %>%
arrange(desc(pct_of_total)) %>%
head(30) %>%
ggplot(aes(x = reorder(word, pct_of_total), y = pct_of_total)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
facet_wrap(~articles.source_name, scales = "free") +
labs(title = "Top Words (with a sentiment)")
words_long %>%
select(word, sentiment, articles.source_name) %>%
drop_na() %>%
group_by(word, articles.source_name) %>%
summarise(count = n()) %>%
group_by(articles.source_name) %>%
mutate(pct_of_total = count / sum(count)) %>%
arrange(desc(pct_of_total)) %>%
head(30)
# top words with a sentiment
words_long %>%
select(word, sentiment, articles.source_name) %>%
drop_na() %>%
group_by(word, articles.source_name) %>%
summarise(count = n()) %>%
group_by(articles.source_name) %>%
mutate(pct_of_total = count / sum(count)) %>%
arrange(desc(pct_of_total)) %>%
head(100) %>%
ggplot(aes(x = reorder(word, pct_of_total), y = pct_of_total)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
facet_wrap(~articles.source_name, scales = "free") +
labs(title = "Top Words (with a sentiment)")
# plot
words_long %>%
group_by(lexicon, sentiment, articles.source_name) %>%
drop_na() %>%
summarise(count = n()) %>%
group_by(lexicon, articles.source_name) %>%
mutate(pct_of_total = count / sum(count)) %>%
ggplot(aes(x = sentiment,
y = pct_of_total,
fill = articles.source_name)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~lexicon, scales = "free") +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "bottom") +
labs(title = "Sentiment Distribution by Word")
# term frequency
words %>%
group_by(word, articles.source_name) %>%
summarise(word_count = n()) %>%
group_by(articles.source_name) %>%
mutate(total_words = sum(word_count)) %>%
ggplot(aes(x = word_count / total_words)) +
geom_histogram() +
facet_wrap(~articles.source_name) +
labs(title = "Word Frequency Distribution", x = "pct of Words")
# tf-idf (for words with a sentiment)
# create custom stop words to remove meaningless words
# reference: https://www.tidytextmining.com/tfidf.html
stop_words_custom <- tibble(word = c("n", "2w7hx9t"))
words_tf_idf <- words_long %>%
anti_join(stop_words_custom) %>%
group_by(word, articles.article_url, articles.source_name) %>%
summarise(word_count = n()) %>%
bind_tf_idf(word, articles.article_url, word_count)
# Highest words by tf_idf
words_tf_idf %>%
arrange(desc(tf_idf)) %>%
head(70) %>%
ggplot(aes(x = reorder(word, tf_idf),
y = tf_idf,
fill = articles.source_name)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip()+
labs(title = "TF-IDF: Top Words", x = "word") +
theme(legend.position = "bottom")
# Highest words by tf_idf
words_tf_idf %>%
arrange(desc(tf_idf)) %>%
head(70) %>%
ggplot(aes(x = reorder(word, tf_idf),
y = tf_idf,
fill = articles.source_name)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
facet_wrap(~articles.source_name) +
labs(title = "TF-IDF: Top Words", x = "word") +
theme(legend.position = "bottom")
# Highest words by tf_idf
words_tf_idf %>%
arrange(desc(tf_idf)) %>%
head(70) %>%
ggplot(aes(x = reorder(word, tf_idf),
y = tf_idf,
fill = articles.source_name)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
facet_wrap(~articles.source_name, scales = "free") +
labs(title = "TF-IDF: Top Words", x = "word") +
theme(legend.position = "bottom")
# words per article
words %>%
group_by(articles.source_name, articles.article_url) %>%
summarise(word_count = n()) %>%
ggplot(aes(x = word_count, fill = articles.source_name)) +
geom_histogram(position = "dodge") +
labs(title = "Word Count Distribution")
# words per article
p1 <- words %>%
group_by(articles.source_name, articles.article_url) %>%
summarise(word_count = n()) %>%
ggplot(aes(x = word_count, fill = articles.source_name)) +
geom_histogram(position = "dodge") +
labs(title = "Word Count Distribution")
p1
# top words with a sentiment
p2 <- words_long %>%
select(word, sentiment, articles.source_name) %>%
drop_na() %>%
group_by(word, articles.source_name) %>%
summarise(count = n()) %>%
group_by(articles.source_name) %>%
mutate(pct_of_total = count / sum(count)) %>%
arrange(desc(pct_of_total)) %>%
head(100) %>%
ggplot(aes(x = reorder(word, pct_of_total), y = pct_of_total)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
facet_wrap(~articles.source_name, scales = "free") +
labs(title = "Top Words (with a sentiment)")
p3
# plot
p3 <- words_long %>%
group_by(lexicon, sentiment, articles.source_name) %>%
drop_na() %>%
summarise(count = n()) %>%
group_by(lexicon, articles.source_name) %>%
mutate(pct_of_total = count / sum(count)) %>%
ggplot(aes(x = sentiment,
y = pct_of_total,
fill = articles.source_name)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~lexicon, scales = "free") +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "bottom") +
labs(title = "Sentiment Distribution by Word")
p3
# term frequency
p5 <- words %>%
group_by(word, articles.source_name) %>%
summarise(word_count = n()) %>%
group_by(articles.source_name) %>%
mutate(total_words = sum(word_count)) %>%
ggplot(aes(x = word_count / total_words)) +
geom_histogram() +
facet_wrap(~articles.source_name) +
labs(title = "Word Frequency Distribution", x = "pct of Words")
p5
# save figures
plot_titles <- c("words_per_article", "top_words_with_sentiment",
"sentiment_distr_by_word", "avg_sentiment_by_week_afinn",
"word_freq_distr", "tfidf_top_words"
)
plots <- list(p1, p2, p3, p4, p5, p6)
# sentiment by week - afinn
p4 <- words_long %>%
drop_na() %>%
mutate(date = floor_date(articles.published_datetime, unit = "week"),
sentiment = as.numeric(sentiment)) %>%
filter(lexicon == "sentiment_afinn") %>%
group_by(date, articles.source_name) %>%
summarise(avg_sentiment = mean(sentiment)) %>%
ggplot(aes(x = date, y = avg_sentiment, color = articles.source_name)) +
geom_line() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Average Sentiment by Week")
p4
p1
p2
p3
colnames(words)
?get_sentiments
for (i in seq_along(plots)) {
ggsave(plot = plots[[i]], file = paste(figures_path,
"word/",
plots_titles,
".png",
sep = ""),
height = 5, width = 5)
}
plots <- list(p1, p2, p3, p4, p5, p6)
# Highest words by tf_idf
p6 <- words_tf_idf %>%
arrange(desc(tf_idf)) %>%
head(70) %>%
ggplot(aes(x = reorder(word, tf_idf),
y = tf_idf,
fill = articles.source_name)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
facet_wrap(~articles.source_name, scales = "free") +
labs(title = "TF-IDF: Top Words", x = "word") +
theme(legend.position = "bottom")
plots <- list(p1, p2, p3, p4, p5, p6)
for (i in seq_along(plots)) {
ggsave(plot = plots[[i]], file = paste(figures_path,
"word/",
plots_titles,
".png",
sep = ""),
height = 5, width = 5)
}
figures_dir <- "../figures/"
ggsave(plot = plots[[i]], file = paste(figures_dir,
"word/",
plots_titles,
".png",
sep = ""),
height = 5, width = 5)
ggsave(plot = plots[[i]], file = paste(figures_dir,
"word/",
plot_titles[i],
".png",
sep = ""),
height = 5, width = 5)
for (i in seq_along(plots)) print(i)
plot_titles[1]
plot_titles[2]
plot_titles[3]
plot_titles[4]
plot_titles[5]
plot_titles[6]
for (i in seq_along(plots)) print( paste(figures_dir,
"word/",
plot_titles[i],
".png",
sep = "")
for (i in seq_along(plots)) print( paste(figures_dir,
"word/",
plot_titles[i],
".png",
sep = ""))
plots[[1]]
plots[[2]]
ggsave(plot = plots[[i]], file = paste(figures_dir,
"word/",
plot_titles[i],
".png",
sep = ""),
height = 10, width = 10)
# words per article
p1 <- words %>%
group_by(articles.source_name, articles.article_url) %>%
summarise(word_count = n()) %>%
ggplot(aes(x = word_count, fill = articles.source_name)) +
geom_histogram(position = "dodge") +
labs(title = "Word Count Distribution") +
theme(legend.position = "bottom")
# top words with a sentiment
p2 <- words_long %>%
select(word, sentiment, articles.source_name) %>%
drop_na() %>%
group_by(word, articles.source_name) %>%
summarise(count = n()) %>%
group_by(articles.source_name) %>%
mutate(pct_of_total = count / sum(count)) %>%
arrange(desc(pct_of_total)) %>%
head(100) %>%
ggplot(aes(x = reorder(word, pct_of_total), y = pct_of_total)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
facet_wrap(~articles.source_name, scales = "free") +
labs(title = "Top Words (with a sentiment)") +
theme(legend.position = "bottom")
# plot
p3 <- words_long %>%
group_by(lexicon, sentiment, articles.source_name) %>%
drop_na() %>%
summarise(count = n()) %>%
group_by(lexicon, articles.source_name) %>%
mutate(pct_of_total = count / sum(count)) %>%
ggplot(aes(x = sentiment,
y = pct_of_total,
fill = articles.source_name)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~lexicon, scales = "free") +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "bottom") +
labs(title = "Sentiment Distribution by Word")
# sentiment by week - afinn
p4 <- words_long %>%
drop_na() %>%
mutate(date = floor_date(articles.published_datetime, unit = "week"),
sentiment = as.numeric(sentiment)) %>%
filter(lexicon == "sentiment_afinn") %>%
group_by(date, articles.source_name) %>%
summarise(avg_sentiment = mean(sentiment)) %>%
ggplot(aes(x = date, y = avg_sentiment, color = articles.source_name)) +
geom_line() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Average Sentiment by Week") +
theme(legend.position = "bottom")
# term frequency
p5 <- words %>%
group_by(word, articles.source_name) %>%
summarise(word_count = n()) %>%
group_by(articles.source_name) %>%
mutate(total_words = sum(word_count)) %>%
ggplot(aes(x = word_count / total_words)) +
geom_histogram() +
facet_wrap(~articles.source_name) +
labs(title = "Word Frequency Distribution", x = "pct of Words") +
theme(legend.position = "bottom")
# Highest words by tf_idf
p6 <- words_tf_idf %>%
arrange(desc(tf_idf)) %>%
head(70) %>%
ggplot(aes(x = reorder(word, tf_idf),
y = tf_idf,
fill = articles.source_name)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
facet_wrap(~articles.source_name, scales = "free") +
labs(title = "TF-IDF: Top Words", x = "word") +
theme(legend.position = "bottom")
p1
p2
p3
p4
p5
p6
# term frequency
p5 <- words %>%
group_by(word, articles.source_name) %>%
summarise(word_count = n()) %>%
group_by(articles.source_name) %>%
mutate(total_words = sum(word_count)) %>%
ggplot(aes(x = word_count / total_words)) +
geom_histogram() +
facet_wrap(~articles.source_name) +
labs(title = "Word Frequency Distribution", x = "pct of Words")
plots
ggsave(plot = plots[[i]], file = paste(figures_dir,
"word/",
plot_titles[i],
".png",
sep = ""),
height = 10, width = 10)
ggsave(plot = p, file = paste(figures_dir,
"word/",
plot_titles[i],
".png",
sep = ""),
height = 10, width = 10)
plots <- list(p1, p2, p3, p4, p5, p6)
for (i in seq_along(plot_titles)) {
p <- plots[[i]]
ggsave(plot = p, file = paste(figures_dir,
"word/",
plot_titles[i],
".png",
sep = ""),
height = 10, width = 10)
}
# save figures
plot_titles <- c("words_per_article", "top_words_with_sentiment",
"word_sentiment_distr", "word_avg_sentiment_by_week_afinn",
"word_freq_distr", "word_tfidf_top_words")
plots <- list(p1, p2, p3, p4, p5, p6)
for (i in seq_along(plot_titles)) {
p <- plots[[i]]
ggsave(plot = p, file = paste(figures_dir,
plot_titles[i],
".png",
sep = ""),
height = 10, width = 10)
}
p1
p2
p3
p4
