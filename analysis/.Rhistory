install.packages("anytime")
library(anytime)
?anytime
anytime(1609179480)
hour(anytime(1609179480))
# filter to columns for modeling
articles_input <- articles %>%
select(c(articles.source_name, avg_sentiment_afinn_word, articles.published_timestamp,
avg_sentiment_afinn_sent, word_count, word_count_sentiment) |
contains("topic")) %>%
mutate(published_hour_et = hour(anytime(articles.published_timestamp)))
articles_input %>% group_by(articles.source_name, published_hour_et) %>% summarise(n())
articles %>% select(articles.published_datetime, articles.published_timestamp, articles.article_url)
articles %>% select(articles.published_datetime, articles.published_timestamp, articles.article_url)
articles %>% select(articles.published_timestamp, articles.article_url)
# filter to columns for modeling
articles_input <- articles %>%
select(c(articles.source_name, avg_sentiment_afinn_word, articles.published_timestamp,
avg_sentiment_afinn_sent, word_count, word_count_sentiment) |
contains("topic")) %>%
mutate(published_hour_et = hour(anytime(articles.published_timestamp)),
published_dow = weekdays(articles.published_datetime))
# filter to columns for modeling
articles_input <- articles %>%
select(c(articles.source_name, avg_sentiment_afinn_word, articles.published_timestamp,
avg_sentiment_afinn_sent, word_count, word_count_sentiment) |
contains("topic")) %>%
mutate(published_hour_et = hour(anytime(articles.published_timestamp)),
published_dow = weekdays(articles$articles.published_datetime))
head(articles_input)
colnames(articles_input)
# filter to columns for modeling
articles_input <- articles %>%
select(c(articles.source_name, avg_sentiment_afinn_word,
articles.published_timestamp, articles.published_datetime
avg_sentiment_afinn_sent, word_count, word_count_sentiment) |
contains("topic")) %>%
mutate(published_hour_et = hour(anytime(articles.published_timestamp)),
published_dow = weekdays(articles$articles.published_datetime))
# filter to columns for modeling
articles_input <- articles %>%
select(c(articles.source_name, avg_sentiment_afinn_word,
articles.published_timestamp, articles.published_datetime,
avg_sentiment_afinn_sent, word_count, word_count_sentiment) |
contains("topic")) %>%
mutate(published_hour_et = hour(anytime(articles.published_timestamp)),
published_dow = weekdays(articles$articles.published_datetime))
head(articles_input)
articles_input %>% select(articles.published_timestamp, articles.published_datetime, published_hour_et, published_dow)
articles_input %>% group_by(articles.source_name, published_dow) %>% summarise(n())
articles_input %>% group_by(articles.source_name, published_dow) %>% summarise(n()) %>% print(n = 100)
articles_input %>% group_by(articles.source_name, published_dow) %>% summarise(n()) %>% print(n = 100) %>% arrange(articles.source_name, desc(`n()`))
articles_input %>% group_by(articles.source_name, published_dow) %>% summarise(n())  %>% arrange(articles.source_name, desc(`n()`))%>% print(n = 100)
colnames(articles_input)
gbm.fit <- gbm(articles.source_name ~ . - articles.published_datetime -
articles.published_timestamp,
data = articles_train,
distribution = "multinomial")
colnames(articles_train)
# train / test split
test_rate <- .2
articles_train <- articles_input %>% sample_frac(size = 1 - test_rate)
articles_test <- articles_input[-c(pull(articles_train %>% select(document))), ]
# inspect distribution of news source
# make sure there is no imbalance across news sources
bind_rows(articles_train %>% mutate(set = "train"),
articles_test %>% mutate(set = "test")) %>%
group_by(articles.source_name, set) %>%
summarise(article_count = n()) %>%
ggplot(aes(x = articles.source_name, y = article_count)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~set)
articles_train <- articles_input %>% sample_frac(size = 1 - test_rate)
articles_test <- articles_input[-c(pull(articles_train %>% select(document))), ]
# filter to columns for modeling
articles_input <- articles %>%
select(c(document, articles.source_name, avg_sentiment_afinn_word,
articles.published_timestamp, articles.published_datetime,
avg_sentiment_afinn_sent, word_count, word_count_sentiment) |
contains("topic")) %>%
mutate(published_hour_et = hour(anytime(articles.published_timestamp)),
published_dow = weekdays(articles$articles.published_datetime))
# train / test split
test_rate <- .2
articles_train <- articles_input %>% sample_frac(size = 1 - test_rate)
articles_test <- articles_input[-c(pull(articles_train %>% select(document))), ]
dim(articles_train)
dim(articles_test)
# inspect distribution of news source
# make sure there is no imbalance across news sources
bind_rows(articles_train %>% mutate(set = "train"),
articles_test %>% mutate(set = "test")) %>%
group_by(articles.source_name, set) %>%
summarise(article_count = n()) %>%
ggplot(aes(x = articles.source_name, y = article_count)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~set)
colnames(articles_train)
gbm.fit <- gbm(articles.source_name ~ . - articles.published_datetime -
articles.published_timestamp - document,
data = articles_train,
distribution = "multinomial")
gimpse(articles_train)
glimpse(articles_train)
# filter to columns for modeling
articles_input <- articles %>%
select(c(document, articles.source_name, avg_sentiment_afinn_word,
articles.published_timestamp, articles.published_datetime,
avg_sentiment_afinn_sent, word_count, word_count_sentiment) |
contains("topic")) %>%
mutate(published_hour_et = hour(anytime(articles.published_timestamp)),
published_dow = weekdays(articles$articles.published_datetime)) %>%
drop_na()
dim(articles)
# train / test split
test_rate <- .2
articles_train <- articles_input %>% sample_frac(size = 1 - test_rate)
articles_test <- articles_input[-c(pull(articles_train %>% select(document))), ]
dim(articles_train)
dim(articles_test)
# inspect distribution of news source
# make sure there is no imbalance across news sources
bind_rows(articles_train %>% mutate(set = "train"),
articles_test %>% mutate(set = "test")) %>%
group_by(articles.source_name, set) %>%
summarise(article_count = n()) %>%
ggplot(aes(x = articles.source_name, y = article_count)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~set)
gbm.fit <- gbm(articles.source_name ~ . - articles.published_datetime -
articles.published_timestamp - document,
data = articles_train %>%
mutate(published_dow = as.factor(published_dow)),
distribution = "multinomial")
library(caret) # Gradient Boosting Machine
install.packages(caret)
install.packages("caret")
library(caret) # Gradient Boosting Machine
?caret
?trainControl
tc <- trainControl(method = "repeatedcv", seeds = seed, number = 10)
gbm.fit <- train(articles.source_name ~ . - articles.published_datetime -
articles.published_timestamp - document,
data = articles_train,
method = "gbm")
gbm.fit <- train(articles.source_name ~ . - articles.published_datetime -
articles.published_timestamp - document,
data = articles_train,
method = "gbm",
trControl = tc)
# git model
tc <- trainControl(method = "repeatedcv", number = 10)
trainControl()
?trainControl
?train
# fit model
gbm.fit <- train(articles.source_name ~ . - articles.published_datetime -
articles.published_timestamp - document,
data = articles_train,
method = "gbm")
# fit model
gbm_fit <- train(articles.source_name ~ . - articles.published_datetime -
articles.published_timestamp - document,
data = articles_train,
method = "gbm")
# predictions on test set
gbm_preds <- predict(gbm_fit, articles_test)
library(gbm)
# fit model
gbm_fit <- gbm(articles.source_name ~ . - articles.published_datetime -
articles.published_timestamp - document,
data = articles_train %>%
mutate(published_dow = as.factor(published_dow)),
distribution = "multinomial")
gbm_fit
gbm_fit
library(ROCR)
dim(articles)
library(anytime)
library(lubridate)
########################## Other features #######################################
articles <- articles %>%
mutate(published_hour_et = hour(anytime(articles.published_timestamp)),
published_dow = weekdays(articles$articles.published_datetime))
# write to csv
write_csv(articles, paste0(data_dir, "news_model_input.csv"))
# write to csv
write_csv(articles, paste0(data_dir, "news_model_input.csv"))
# not run
# setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/analysis")
library(anytime) # epoch to datetime
library(bartCause) # Bayesian Additive Regression Trees
library(dplyr)
library(caret) # Gradient Boosting Machine
library(gbm) # Gradient Boosting Machine
library(lubridate)
library(readr)
library(ROCR)
seed <- 14
set.seed(seed)
# Purpose: Predict the news source
data_dir <- "../data/"
figures_dir <- "../figures/"
articles <- read_csv(paste0(data_dir, "news_model_input.csv"))
# filter to columns for modeling
articles_input <- articles %>%
select(c(document, articles.source_name, avg_sentiment_afinn_word,
articles.published_timestamp, articles.published_datetime,
avg_sentiment_afinn_sent, word_count, word_count_sentiment,
published_hour_et, published_dow) |
contains("topic")) %>%
drop_na()
dim(articles_input)
dim(articles)
# not run
# setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/analysis")
library(anytime)
library(dplyr)
library(ggplot2)
library(lubridate)
library(readr)
library(tidyr)
library(tidytext)
library(topicmodels)
seed <- 14
# not run
# setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/analysis")
library(anytime)
library(dplyr)
library(ggplot2)
library(lubridate)
library(readr)
library(tidyr)
library(tidytext)
library(topicmodels)
seed <- 14
# directories
figures_dir <- "../figures/"
data_dir <- "../data/"
# read news data
articles <- read_csv(paste0(data_dir, "news_all.csv"))  %>%
mutate(document = as.character(row_number()))
# filter out international Reuters sources
news_source_exclude <- c("Reuters Africa", "Reuters Australia",
"Reuters India", "Reuters UK", "Reuters.com")
# remove international Reuters articles
articles <- filter(articles, !articles.source_name %in% news_source_exclude)
dim(articles)
# train / test split
test_rate <- .2
articles_train <- articles_input %>% sample_frac(size = 1 - test_rate)
articles_test <- articles_input[-c(pull(articles_train %>% select(document))), ]
# inspect distribution of news source
# make sure there is no imbalance across news sources
bind_rows(articles_train %>% mutate(set = "train"),
articles_test %>% mutate(set = "test")) %>%
group_by(articles.source_name, set) %>%
summarise(article_count = n()) %>%
ggplot(aes(x = articles.source_name, y = article_count)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~set)
dim(articles)
dim(articles_input)
# gbm package: fit model
# multi-class is ill-advised per documentation
gbm_fit <- gbm(articles.source_name ~ . - articles.published_datetime -
articles.published_timestamp - document,
data = articles_train %>%
mutate(published_dow = as.factor(published_dow)),
distribution = "multinomial")
gbm_fit
gbm_fit <- train(articles.source_name ~ . - articles.published_datetime -
articles.published_timestamp - document,
data = articles_train,
method = "gbm")
# predictions on test set
gbm_preds <- predict(gbm_fit, articles_test)
gbm_preds
gbm_accuracy <- mean(gbm_preds == articles_train$articles.source_name)
gbm_accuracy <- mean(gbm_preds == articles_test$articles.source_name)
gbm_accuracy
gbm_preds
predict(gbm_fit, articles_test, type = "response")
predict(gbm_fit, articles_test, type = "prob")
# probabilities on test set
gbm_prob <- predict(gbm_fit, articles_test, type = "prob")
gbm_prob
library(pROC)
print("GBM Accuracy:", round(gbm_accuracy, 1)
print("GBM Accuracy:", round(gbm_accuracy, 1))
gbm_accuracy <- mean(gbm_preds == articles_test$articles.source_name)
print("GBM Accuracy:", round(gbm_accuracy, 1))
paste("GBM Accuracy:", round(gbm_accuracy, 1))
paste("GBM Accuracy:", round(gbm_accuracy, 3))
dim(articles_train)
dim(articles_test)
colnames(articles_train)
gbm_fit
?hour
anytime(1609179480)
hour(anytime(1609179480))
articles %>%
mutate(published_hour_et = hour(anytime(articles.published_timestamp)),
published_dow = weekdays(articles$articles.published_datetime))
articles %>% mutate(hour(anytime(articles.published_timestamp)))
articles %>% mutate(hour(anytime(articles.published_timestamp))) %>% select(articles.published_timestamp, published_hour_et)
articles %>% mutate(published_hour_et = hour(anytime(articles.published_timestamp))) %>% select(articles.published_timestamp, published_hour_et)
articles %>% mutate(published_hour_et = hour(anytime(articles.published_timestamp))) %>% select(articles.published_timestamp, published_hour_et) %>% distinct()
articles %>% mutate(published_hour_et = hour(anytime(articles.published_timestamp))) %>% select(articles.published_timestamp, published_hour_et)
articles %>% mutate(published_hour_et = hour(anytime(articles.published_timestamp))) %>% select(articles.published_timestamp, published_hour_et) %>% group_by(published_hour_et) %>% summarise(n()) %>% arrange(desc(`n()`))
attributes(gbm_fit)
gbm_fit$coefnames
gbm_fit$pred
gbm_fit$results
varImp(gbm_fit)
# plot variable importance
class(varImp(gbm_fit))
# plot variable importance
tible(varImp(gbm_fit))
# plot variable importance
tibble(varImp(gbm_fit))
# plot variable importance
as.data.frame(varImp(gbm_fit))
# plot variable importance
varImp(gbm_fit)
# plot variable importance
varImp(gbm_fit, scale = FALSE)
?bartc
library(xgboost)
install.packages(xgboost)
install.packages("xgboost")
library(xgboost)
?BART
library(BART)
?BART
library(xgboost)
gbm_preds
summary(gbm_preds)
train_xgb <- xgb.DMatrix(data = as.matrix(articles_train %>%
select(-articles.source_name)),
label = as.matrix(articles_train$articles.source_name))
train_xgb <- xgb.DMatrix(data = as.matrix(
articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-articles.source_name)),
label = as.matrix(articles_train$articles.source_name))
articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-articles.source_name)
train_xgb <- xgb.DMatrix(data = as.matrix(
articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-c(articles.source_name, document,
articles.published_datetime, articles.published_timestamp)
)
),
label = as.matrix(articles_train$articles.source_name))
articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-c(articles.source_name, document,
articles.published_datetime, articles.published_timestamp)
)
articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-articles.source_name)) %>% colnames
articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-c(articles.source_name, document,
articles.published_datetime, articles.published_timestamp)
) %>% colnames
glimpse(  articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-c(articles.source_name, document,
articles.published_datetime, articles.published_timestamp)
))
as.matrix(
articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-c(articles.source_name, document,
articles.published_datetime, articles.published_timestamp,
published_hour_et)
)
)
train_xgb <- xgb.DMatrix(data = as.matrix(
articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-c(articles.source_name, document,
articles.published_datetime, articles.published_timestamp,
published_hour_et)
)
),
label = as.matrix(
articles.source_name = as.factor(
articles_train$articles.source_name)
)
)
as.matrix(
articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-c(articles.source_name, document,
articles.published_datetime, articles.published_timestamp,
published_hour_et)
)
)
as.matrix(
articles.source_name = as.factor(
articles_train$articles.source_name)
)
as.matrix(
articles.source_name = as.factor(
articles_train$articles.source_name)
)
as.matrix(
articles.source_name = as.factor(
articles_train$articles.source_name)
)
train_xgb <- xgb.DMatrix(data = as.matrix(
articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-c(articles.source_name, document,
articles.published_datetime, articles.published_timestamp,
published_hour_et)
)
),
label = as.matrix(as.factor(articles_train$articles.source_name))
)
as.matrix(as.factor(articles_train$articles.source_name)
)
train_xgb <- xgb.DMatrix(data = as.matrix(
articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-c(articles.source_name, document,
articles.published_datetime, articles.published_timestamp,
published_hour_et)
)
),
label = as.matrix(as.factor(articles_train$articles.source_name))
)
articles[ , "articles.source_name"]
unique(articles_train$articles.source_name)
unique(articles$articles.source_name)
sort(unique(articles$articles.source_name))
sort(unique(articles$articles.source_name))
# convert class labels for xgboost
news_source <- list(1:nrow(sort(unique(articles$articles.source_name))))
nrow(sort(unique(articles$articles.source_name)))
# convert class labels for xgboost
news_source <- list(1:length(sort(unique(articles$articles.source_name))))
news_source - 1
news_source
# convert class labels for xgboost
news_source <- as.vector(1:length(sort(unique(articles$articles.source_name)))))
# convert class labels for xgboost
news_source <- as.vector(1:length(sort(unique(articles$articles.source_name))))
news_source
# convert class labels for xgboost
news_source <- as.vector(1:length(sort(unique(articles$articles.source_name)))) - 1
news_source
# convert class labels for xgboost
news_source_unique <- sort(unique(articles$articles.source_name))
class_label <- as.vector(1:length(news_source_unique)) - 1
class_label
names(news_sources) <- news_source_unique
names(class_label) <- news_source_unique
class_label
class_label[1]
class_label[1] + 1
train_xgb <- xgb.DMatrix(data = as.matrix(
articles_train %>%
mutate(published_dow = as.factor(published_dow)) %>%
select(-c(articles.source_name, document,
articles.published_datetime, articles.published_timestamp,
published_hour_et)
)
),
label = as.matrix(class_label)
)
# plot variable importance
varImp(gbm_fit)
install.packages("mltools")
library(mltools) # one-hot encoding for xgboost
one_hot(as.data.table(articles_train$published_dow)))
one_hot(as.data.table(articles_train$published_dow))
one_hot(articles_train$published_dow)
one_hot(as.factor(articles_train)$published_dow)
one_hot(as.factor(articles_train$published_dow))
class(one_hot(as.factor(articles_train$published_dow)))
dim(one_hot(as.factor(articles_train$published_dow))
dim(one_hot(as.factor(articles_train$published_dow)))
d <- dummyVars(" ~ ,", data = articles_train %>% select(articles.source_name))
d <- dummyVars(" ~ .", data = articles_train %>% select(articles.source_name))
d
data.frame(predict(d, newdata = articles_train %>% select(articles.source_name)))
class_label <- data.frame(predict(d, newdata = articles_train %>% select(articles.source_name)))
class_label
dim(class_label)
d
d$vars
d$facVars
d$call
d$form
d$lvls
d$terms
d$fullRank
d$call
d$form
d$vars
d$facVars
d
d$facVars
class_label
sum(class_label)
