# 2. Load and filter data #
###########################
figures_path <- "../figures/"
sqf <- read_csv("../data/sqf_08_16.csv")
# not run
setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/HW4/scripts")
library(lubridate)
library(ROCR)
library(tidyverse)
library(rvest)
set.seed(2048)
###########################
# 2. Load and filter data #
###########################
figures_path <- "../figures/"
sqf <- read_csv("../data/sqf_08_16.csv")
dim(all_sqf_data)
all_sqf_data <- sqf %>%
filter(suspected.crime == "cpw" & precinct != 121) %>%
select(contains("additional.") |
contains("stopped.bc") |
c(id, year, found.weapon, precinct,
location.housing, suspect.age, suspect.build, suspect.sex,
suspect.height, suspect.weight, inside,
radio.run, officer.uniform, observation.period,
day, month, time.period)) %>%
mutate(time.period = as.factor(time.period),
precinct = as.factor(precinct)) %>%
drop_na()
dim(sqf)
length(unique(sqf$id))
# not run
# setwd('C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/HW4/scripts')
library(lubridate)
library(rvest)
library(tidyverse)
############################
# 2. Scrape and Clean Data #
############################
figures_path <- "../figures/"
################################## functions ###################################
url_format <- function(x) {
# get urls given a vector of neighborhoods, x
# formatted neighborhood name
format_ <- function(x) str_replace(str_to_lower(x), " ", "-")
# fetch url
domain <- "https://www.universalhub.com/crime/"
u <- case_when(x == "Chinatown" ~ str_replace(domain, "crime", "newcrime/9967"),
x == "Mission Hill" ~ paste0(domain, format_(x)),
TRUE ~ paste0(domain, format_(x), ".html")
)
return(u)
}
fetch_table <- function(x) {
# fetch table from html page given neighborhood, x
# format url
url_ <- url_format(x)
# fetch all tables in a neighborhood
tables <- list()
page <- 0
rows <- 1
i <- 1
while (rows > 0) {
t <- as.data.frame(
read_html(paste0(url_, "?page=", page)) %>% html_table()
)
if (nrow(t) > 0) {
t$nbhd <- x
tables[[i]] <- t
page <- page + 1
i <- i + 1
} else break
}
# combine pages for neighborhood
crimes <- bind_rows(tables)
# hour of crime
crimes$hour <- hour(parse_date_time(crimes$Date, "%m/%d/%y - %H:%M %p"))
return(crimes)
}
############################### scrape crimes ##################################
# neighborhoods
nb <- c("Allston", "Back Bay", "Beacon Hill", "Brighton", "Charlestown",
"Chinatown", "Dorchester", "Downtown", "East Boston", "Fenway",
"Hyde Park", "Jamaica Plain", "Mattapan", "Mission Hill",
"North End", "Roslindale", "Roxbury", "South Boston",
"South End", "West Roxbury")
# scrape crimes data
crime_data <- bind_rows(lapply(nb, fetch_table))
# clean up crime names, include columns of interest
crime_data <- crime_data %>%
mutate(crime = case_when((Type == "Shoting") |
(Type == "Shotting") ~ "Shooting",
Type == "gunfire" ~ "Gunfire",
TRUE ~ Type)
) %>%
select(crime, hour, nbhd)
# checks
paste0("# of Crimes: ", nrow(crime_data))
paste0("# of Neighborhoods: ", crime_data %>%
select(nbhd) %>%
distinct() %>%
nrow())
paste0("# of Fields: ", ncol(crime_data))
head(crime_data)
domain <- "https://www.universalhub.com/crime/"
t <- read_html(domain)
t %>% html_nodes()
t %>% html_nodes("body")
t %>% html_nodes("body") %>% xml_find_all("//select[contains(@class, 'd-1')]")
t %>% html_nodes("body") %>% xml_find_all("//select[contains(@class, 'd-1')]")
t %>% html_nodes("body") %>% xml_find_all("//option[contains(@value, 'd-1')]")
getwd()
getwd()
setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/results")
knitr::opts_chunk$set(echo = TRUE)
# not run
# setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/scripts")
library(httr)
library(jsonlite)
library(lubridate)
library(rvest)
library(tidyverse)
library(tidytext)
################
# Get API data #
################
# API Documentation: https://documenter.getpostman.com/view/12365554/TVep87Q1#intro
# note: generate your own GNews API token and save it as a file called "api_token"
token <- colnames(read.csv("../api_token"))
# parameters
topic <- "\"COVID-19\""
country <- "us"
language <- "en"
# article limit per API call under the free plan
limit <- "20"
source_c <- "cnn"
source_r <- "reuters"
# date range
date_init <- as.Date("2020-01-01")
date_end <- as.Date("2021-04-09")
get_date_range <- function(date_start, date_end, start_end = "start") {
# get start or end of week for each date, given range of dates
# date_start: beginning date
# date_end: end date
# start_end: Get first or last date of the week. Values: "start" or "end"
dates <- integer(0)
class(dates) <- "Date"
i <- 1
while (date_start <= date_end) {
if (start_end == "start")
{dates[i] <- floor_date(date_start, unit = "week")}
else
{dates[i] <- ceiling_date(date_start, unit = "week") - 1}
date_start <- date_start + 7
i <- i + 1
}
return(dates)
}
# date ranges for API
dates_start <- get_date_range(date_init, date_end, start_end = "start")
dates_end <- get_date_range(date_init, date_end, start_end = "end")
dates_start
length(dates_start)
length(dates_end)
67 * 20 * 4
library(anytime)
# not run
# setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/results")
library(anytime)
library(ggplot2)
library(dplyr)
library(lubridate)
library(readr)
library(stringr)
library(tidyr)
library(tidytext)
library(topicmodels)
seed <- 14
# not run
# setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/analysis")
library(ggplot2)
library(dplyr)
library(lubridate)
library(readr)
# library(reshape2)
library(stringr)
library(tidyr)
library(tidytext)
library(topicmodels)
seed <- 14
figures_dir <- "../figures/"
articles %>%
group_by(articles.source_name) %>%
summarise(articles = n()) %>%
arrange(desc(articles))
data_dir <- "../data/"
articles <- read_csv(paste0(data_dir, "news_all.csv"))
paste0(data_dir, "news_all.csv")
# not run
setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/analysis")
articles <- read_csv(paste0(data_dir, "news_all.csv"))
dim(articles)
# number of articles by source
articles %>%
group_by(articles.source_name) %>%
summarise(articles = n()) %>%
arrange(desc(articles))
tokenize_words <- function(df, text) {
# tokenize words and add sentiments given news dataframe
# text: column to tokenize
words <- df %>%
unnest_tokens(word, text) %>%
left_join(get_sentiments(lexicon = "bing") %>%
mutate(sentiment_bing = sentiment) %>%
select(-sentiment)) %>%
left_join(get_sentiments(lexicon = "afinn") %>%
mutate(sentiment_afinn = as.factor(value)) %>%
select(-value)) %>%
left_join(get_sentiments(lexicon = "loughran") %>%
mutate(sentiment_loughran = sentiment) %>%
select(-sentiment)) %>%
left_join(get_sentiments(lexicon = "nrc") %>%
mutate(sentiment_nrc = sentiment) %>%
select(-sentiment))
return(words)
}
# baseline: tokenize words
words <- tokenize_words(articles, text)
# words per article
p1 <- words %>%
group_by(articles.source_name, articles.article_url) %>%
summarise(word_count = n()) %>%
ggplot(aes(x = word_count, fill = articles.source_name)) +
geom_histogram(position = "dodge") +
labs(title = "Word Count Distribution") +
theme(legend.position = "bottom")
p1
articles <- read_csv(paste0(data_dir, "news_all.csv")) %>%
filter((articles.source_domain == "www.reuters.com") &
(articles.source_name == "Reuters")) %>%
mutate(articles.source_name =
if_else(articles.source_name == "Wall Street Journal",
"The Wall Street Journal",
articles.source_name))
dim(articles)
# read cnn and reuters. Drop International Reuters news sources
articles_cnn_reuters <- bind_rows(read_csv("../data/news_data_cnn.csv"),
read_csv("../data/news_data_reuters.csv") %>%
filter(
(articles.source_domain == "www.reuters.com")
&
(articles.source_name == "Reuters"))
)
# read wsj and bbc
articles_bbc_wsj <- bind_rows(read_csv("../data/news_wsj.csv"),
read_csv("../data/news_bbc.csv"))
# remove first three columns and consolidate source names
articles_bbc_wsj <- articles_bbc_wsj %>%
select(-c(X1, X.1, X)) %>%
mutate(articles.source_name =
if_else(articles.source_name == "Wall Street Journal",
"The Wall Street Journal",
articles.source_name))
# combine articles
articles <- bind_rows(articles_cnn_reuters, articles_bbc_wsj)
# read wsj and bbc
articles_bbc_wsj <- bind_rows(read_csv("../data/news_wsj.csv"),
read_csv("../data/news_bbc.csv"))
# read wsj and bbc
articles_bbc_wsj <- bind_rows(read_csv("../data/news_wsj_new.csv"),
read_csv("../data/news_bbc.csv"))
articles_bbc_wsj <- articles_bbc_wsj %>%
select(-c(X1, X.1, X)) %>%
mutate(articles.source_name =
if_else(articles.source_name == "Wall Street Journal",
"The Wall Street Journal",
articles.source_name))
# combine articles
articles <- bind_rows(articles_cnn_reuters, articles_bbc_wsj)
dim(articles)
# number of articles by source
articles %>%
group_by(articles.source_name) %>%
summarise(articles = n()) %>%
arrange(desc(articles))
tokenize_words <- function(df, text) {
# tokenize words and add sentiments given news dataframe
# text: column to tokenize
words <- df %>%
unnest_tokens(word, text) %>%
left_join(get_sentiments(lexicon = "bing") %>%
mutate(sentiment_bing = sentiment) %>%
select(-sentiment)) %>%
left_join(get_sentiments(lexicon = "afinn") %>%
mutate(sentiment_afinn = as.factor(value)) %>%
select(-value)) %>%
left_join(get_sentiments(lexicon = "loughran") %>%
mutate(sentiment_loughran = sentiment) %>%
select(-sentiment)) %>%
left_join(get_sentiments(lexicon = "nrc") %>%
mutate(sentiment_nrc = sentiment) %>%
select(-sentiment))
return(words)
}
# baseline: tokenize words
words <- tokenize_words(articles, text)
# words per article
p1 <- words %>%
group_by(articles.source_name, articles.article_url) %>%
summarise(word_count = n()) %>%
ggplot(aes(x = word_count, fill = articles.source_name)) +
geom_histogram(position = "dodge") +
labs(title = "Word Count Distribution") +
theme(legend.position = "bottom")
p1
---
title: "Measuring Bias in COVID-19 News Articles"
author: "Andrew Pagtakhan, Kwan Bo Shim, Jazmin Trejo"
date: "May 10th, 2021"
output:
pdf_document:
toc: yes
toc_depth: 3
number_section: true
extra_dependencies: ["float"]
---
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
install.packages(flafter)
colnames(articles)
# not run
# setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/analysis")
library(ggplot2)
library(dplyr)
library(lubridate)
library(readr)
# library(reshape2)
library(stringr)
library(tidyr)
library(tidytext)
library(topicmodels)
seed <- 14
figures_dir <- "../figures/"
################ SENTIMENT ANALYSIS ############################################
# read cnn and reuters. Drop International Reuters news sources
articles_cnn_reuters <- bind_rows(read_csv("../data/news_data_cnn.csv"),
read_csv("../data/news_data_reuters.csv") %>%
filter(
(articles.source_domain == "www.reuters.com")
&
(articles.source_name == "Reuters"))
)
# read wsj and bbc
articles_bbc_wsj <- bind_rows(read_csv("../data/news_wsj_new.csv"),
read_csv("../data/news_bbc.csv")) %>%
mutate(text = text_new)
# remove first three columns and consolidate source names
articles_bbc_wsj <- articles_bbc_wsj %>%
select(-c(X1, X.1, X, X.2)) %>%
mutate(articles.source_name =
if_else(articles.source_name == "Wall Street Journal",
"The Wall Street Journal",
articles.source_name))
# combine articles
articles <- bind_rows(articles_cnn_reuters, articles_bbc_wsj)
colnames(articles)
# not run
# setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/analysis")
library(ggplot2)
library(dplyr)
library(lubridate)
library(readr)
# library(reshape2)
library(stringr)
library(tidyr)
library(tidytext)
library(topicmodels)
seed <- 14
figures_dir <- "../figures/"
################ SENTIMENT ANALYSIS ############################################
# read cnn and reuters. Drop International Reuters news sources
articles_cnn_reuters <- bind_rows(read_csv("../data/news_data_cnn.csv"),
read_csv("../data/news_data_reuters.csv") %>%
filter(
(articles.source_domain == "www.reuters.com")
&
(articles.source_name == "Reuters"))
)
# read wsj and bbc
articles_bbc_wsj <- bind_rows(read_csv("../data/news_wsj_new.csv"),
read_csv("../data/news_bbc.csv")) %>%
mutate(text = text_new) %>%
select(-text_new)
# remove first three columns and consolidate source names
articles_bbc_wsj <- articles_bbc_wsj %>%
select(-c(X1, X.1, X, X.2)) %>%
mutate(articles.source_name =
if_else(articles.source_name == "Wall Street Journal",
"The Wall Street Journal",
articles.source_name))
# combine articles
articles <- bind_rows(articles_cnn_reuters, articles_bbc_wsj)
colnames(articles)
dim(articles)
# number of articles by source
articles %>%
group_by(articles.source_name) %>%
summarise(articles = n()) %>%
arrange(desc(articles))
tokenize_words <- function(df, text) {
# tokenize words and add sentiments given news dataframe
# text: column to tokenize
words <- df %>%
unnest_tokens(word, text) %>%
left_join(get_sentiments(lexicon = "bing") %>%
mutate(sentiment_bing = sentiment) %>%
select(-sentiment)) %>%
left_join(get_sentiments(lexicon = "afinn") %>%
mutate(sentiment_afinn = as.factor(value)) %>%
select(-value)) %>%
left_join(get_sentiments(lexicon = "loughran") %>%
mutate(sentiment_loughran = sentiment) %>%
select(-sentiment)) %>%
left_join(get_sentiments(lexicon = "nrc") %>%
mutate(sentiment_nrc = sentiment) %>%
select(-sentiment))
return(words)
}
# baseline: tokenize words
words <- tokenize_words(articles, text)
# words per article
p1 <- words %>%
group_by(articles.source_name, articles.article_url) %>%
summarise(word_count = n()) %>%
ggplot(aes(x = word_count, fill = articles.source_name)) +
geom_histogram(position = "dodge") +
labs(title = "Word Count Distribution") +
theme(legend.position = "bottom")
p1
# not run
# setwd("C:/Users/apagta950/Documents/NYU/Courses/Spring 2021/MDML/Final Project/US-News-NLP/analysis")
library(ggplot2)
library(dplyr)
library(lubridate)
library(readr)
# library(reshape2)
library(stringr)
library(tidyr)
library(tidytext)
library(topicmodels)
seed <- 14
figures_dir <- "../figures/"
################ SENTIMENT ANALYSIS ############################################
# read cnn and reuters. Drop International Reuters news sources
articles_cnn_reuters <- bind_rows(read_csv("../data/news_data_cnn.csv"),
read_csv("../data/news_data_reuters.csv") %>%
filter(
(articles.source_domain == "www.reuters.com")
&
(articles.source_name == "Reuters"))
)
# read wsj and bbc
articles_bbc_wsj <- bind_rows(read_csv("../data/news_wsj_new.csv"),
read_csv("../data/news_bbc.csv")) %>%
select(-text_new)
# remove first three columns and consolidate source names
articles_bbc_wsj <- articles_bbc_wsj %>%
select(-c(X1, X.1, X, X.2)) %>%
mutate(articles.source_name =
if_else(articles.source_name == "Wall Street Journal",
"The Wall Street Journal",
articles.source_name))
# combine articles
articles <- bind_rows(articles_cnn_reuters, articles_bbc_wsj)
###############################
# Baseline: Sentiment by Word #
###############################
# number of articles by source
articles %>%
group_by(articles.source_name) %>%
summarise(articles = n()) %>%
arrange(desc(articles))
tokenize_words <- function(df, text) {
# tokenize words and add sentiments given news dataframe
# text: column to tokenize
words <- df %>%
unnest_tokens(word, text) %>%
left_join(get_sentiments(lexicon = "bing") %>%
mutate(sentiment_bing = sentiment) %>%
select(-sentiment)) %>%
left_join(get_sentiments(lexicon = "afinn") %>%
mutate(sentiment_afinn = as.factor(value)) %>%
select(-value)) %>%
left_join(get_sentiments(lexicon = "loughran") %>%
mutate(sentiment_loughran = sentiment) %>%
select(-sentiment)) %>%
left_join(get_sentiments(lexicon = "nrc") %>%
mutate(sentiment_nrc = sentiment) %>%
select(-sentiment))
return(words)
}
# baseline: tokenize words
words <- tokenize_words(articles, text)
# words per article
p1 <- words %>%
group_by(articles.source_name, articles.article_url) %>%
summarise(word_count = n()) %>%
ggplot(aes(x = word_count, fill = articles.source_name)) +
geom_histogram(position = "dodge") +
labs(title = "Word Count Distribution") +
theme(legend.position = "bottom")
p1
getwd()
